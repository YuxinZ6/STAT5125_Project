---
title: "STAT5125_Project_Yuxin_Jen_Rene"
author: "Yuxin Zhang, Jennifer Nguyen, Rene Chang"
date: "2023-03-31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Importing and Cleaning
## Set Up Environment
```{r environments and setup, message=FALSE}
# calling required packages
library(readr)
library(tidyverse)
library(tidymodels)
library(randomForest)
library(parsnip)
library(workflows)
library(parallel)
library(doParallel)
library(yardstick)
library(ggplot2)
library(xgboost)
library(yardstick)
library(kknn)
library(poissonreg)
tidymodels_prefer()
```

## Read in Data and Cleaning
### Cleaning Dataset 11_Direct_Investment-related_Indicators
```{r Cleaning dataset 11_Direct_Investment-related_Indicators}
####### Cleaning dataset 11_Direct_Investment-related_Indicators #######
# read the investment indicators dataset
investment_indicators <- read_csv("11_Direct_Investment-related_Indicators.csv")

# initial investigation in the dataset. 
investment_indicators %>% glimpse()
# subset the dataset to discard unwanted columns
investment_indicators <- investment_indicators %>% 
  select(Country, ISO2, ISO3, Indicator, Unit, `CTS Code`, `CTS Name`, Sector,
         `2005`:`2018`)
investment_indicators %>% glimpse()


# creating dictionary for the country with ISO2 and ISO3. 
Country_label <- investment_indicators %>% select(Country, ISO2, ISO3) %>% distinct()

# creating dictionary for the sectors.
Sectors_label <- investment_indicators %>% select(Sector) %>% distinct()
Sectors_label <- Sectors_label %>% 
  mutate(label = sapply(1:length(Sector), function(i) paste0("sector_", i)))

# creating dictionary for the 8 types of CO2 emissions. 
CTS_label <- investment_indicators %>% select(`CTS Code`, `CTS Name`) %>% distinct()

# creating dictionary for unit
unique(investment_indicators$Unit)
Unit_label <- matrix(c("Metric tons of CO2", "MT", 
                       "Metric tons of CO2 per million US$ of output", "MTPMDO",
                       "Metric tons per million US$", "MTPMD"), 
                     ncol = 2, byrow = TRUE) %>% as.data.frame()
colnames(Unit_label) <- c("Unit", "Unit_abb")


# match Units in dataset with its abbreviations, using the dictionary
# match Sectors in dataset with its abbreviations, using the dictionary
investment_indicators <- investment_indicators %>% 
  mutate(Unit_abb = sapply(Unit, 
                           function(x) Unit_label$Unit_abb[match(x, Unit_label$Unit)])) %>%
  relocate(Unit_abb, .before = Unit) %>% 
  mutate(Sector_abb = sapply(Sector, 
                             function(x) Sectors_label$label[match(x, Sectors_label$Sector)])) %>%
  relocate(Sector_abb, .before = Sector) %>% 
  glimpse()

# select the columns of interest, save it temporarily.
tmp <- investment_indicators %>% select(ISO3, `CTS Code`, Sector_abb, 
                                        `2005`:`2018`)
# pivot longer the year and pivot wider the indicators, save as emission_df
emission_df <- tmp %>% pivot_longer(cols = `2005`:`2018`,
                                    names_to = "Year",
                                    values_to = "CO2_emission") %>%
  pivot_wider(names_from = `CTS Code`,
              values_from = CO2_emission) %>% 
  select(-ECBIPF, -ECBIPD, -ECBIFR, -ECBIFF) %>% 
  mutate_all(~ ifelse(is.na(.), 0, .))

# Combining some sectors that have overlaps
emission_df <- emission_df %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_1", 
             "sector_14"),
             "Food", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_18", 
             "sector_42", "sector_43", "sector_44"),
             "Mining", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_2", 
             "sector_40"),
             "Agriculture", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_3", 
             "sector_37"),
             "Recreation", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_4", 
             "sector_12","sector_22", "sector_34"),
             "RawMaterial", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_5", 
             "sector_46","sector_38"),
             "Pharma", Sector_abb)) %>%  
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_6", 
             "sector_11", "sector_39", "sector_50"),
             "Energy", Sector_abb)) %>% 
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_7", 
             "sector_10", "sector_30"),
             "Electronic", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_8"),
             "Construction", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_9"),
             "Education", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_13",
             "sector_20", "sector_28", "sector_25", "sector_35",
             "sector_45", "sector_47", "sector_48", "sector_15",
             "sector_16", "sector_27", "sector_26"),
             "HumanActivities", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_17", 
             "sector_21"),
             "Machinery", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_19", 
             "sector_23", "sector_32", "sector_33", "sector_36",
             "sector_41", "sector_49", "sector_51"),
             "Transportation", Sector_abb)) %>%
  mutate(Sector_abb = if_else(Sector_abb %in% c("sector_24", 
             "sector_29", "sector_31"),
             "Manufacturing", Sector_abb)) %>%
  group_by(ISO3, Year, Sector_abb) %>%
  reframe(across(c(ECBIXD, ECBIXF, ECBIOD, ECBIOF), sum))
  

# remove data that will not be used again
rm(tmp, investment_indicators)



### Summary: all dictionaries have ending with "_label". 
### We will be using emission_df for further analysis. 
```

### Cleaning Dataset 24_Climate-related_Diasters_Frequency
```{r Cleaning dataset 24_Climate-related_Diasters_Frequency}
####### Cleaning dataset 24_Climate-related_Diasters_Frequency#######
disasters_frequency <- read_csv("24_Climate-related_Disasters_Frequency.csv")

# CTS Code for climate related disasters frequency is ECCD. 
# Adding ECCD to CTS_label dictionary. 
# Obtain the CTS code and name for disasters
ECCD_code <- disasters_frequency$`CTS Code` %>% unique()
ECCD_name <- disasters_frequency$`CTS Name` %>% unique()
# adding the CTS code and name for disasters to CTS dictionary
ECCD_df <- data.frame(ECCD_code, ECCD_name)
names(ECCD_df) <- c("CTS Code", "CTS Name")
CTS_label <- rbind(CTS_label, ECCD_df)


# Subset the dataset to discard unwanted data. 
# Narrow down the data to focus on years from 2005 to 2018 (range of years
# in the emission_df).
disasters_frequency <- disasters_frequency %>%
  select(ISO3, Indicator, `CTS Code`, `2005`:`2018`)

# mutate the Indicator column to trim away repeated information
disasters_frequency <- disasters_frequency %>% 
  mutate(Indicator_abb = 
           str_extract(Indicator, "(?<=: ).*")) %>%
  relocate(Indicator_abb, .after = Indicator) %>%
  select(-Indicator) %>%
  rename(Indicator = Indicator_abb)

# filter to only total count of disasters.
disasters_frequency <- disasters_frequency %>%
  filter(Indicator == "TOTAL") %>% 
  select(-Indicator)

# replace all na with zeros
disasters_frequency <- disasters_frequency %>%
  mutate_all(~replace_na(., 0)) %>% glimpse()

disasters_df <- disasters_frequency %>% 
  pivot_longer(cols = `2005`:`2018`, 
               names_to = "Year",
               values_to = "Count") %>%
  pivot_wider(names_from = `CTS Code`, 
              values_from = Count)



#######Combining the two data sets#######
df <- emission_df %>% left_join(disasters_df, by = c("ISO3", "Year"))
df %>% 
  mutate_at("Year", as.factor) %>%
  glimpse()

# remove data not going to be used
rm(ECCD_df, disasters_frequency)



#####More Manipulation to the Dataset#####
df <- df %>% relocate(Year, .before = Sector_abb)
df <- df %>% mutate_all(~ ifelse(is.na(.), 0, .))
df <- df %>% rename(Export_Domestics = ECBIXD,
                           Export_Foreign = ECBIXF,
                           Output_Domestic = ECBIOD,
                           Output_Foreign = ECBIOF,
                           Disaster_Frequency = ECCD)


df_long <- df %>% pivot_longer(cols = Export_Domestics:Output_Foreign,
                           names_to = "Category",
                           values_to = "Value")
df_wide <- df_long %>% pivot_wider(names_from = c("Sector_abb", "Category"), 
                                   values_from = Value, 
                                   values_fill = 0,
                                   names_glue = "{Category}_{Sector_abb}")

# convert year from character to factor
df_wide <- df_wide %>% 
  mutate(Year = as.factor(Year))
# rm(df_long, df)
```

## Train Test Splitting
```{r Train Test Splitting}
set.seed(123457)
df_split <- df_wide %>% initial_split(prop = 0.8)
train <- df_split %>% training()
test <- df_split %>% testing()
```

# Defining the Research Question
To examine the effects of domestically- versus foreign- controlled enterprises' CO2 emissions relative to sectors of activities performed for each country on climate related disasters. 
The models that we will approach are as the following:
- (Rene) Poisson Generalized Linear Regression Model with Lasso Penalty
  - Lasso Penalty will be determined through cross validation. 
- (Rene) Poisson Generalized Linear Model with Elastic Net Penalty
  - cross validation to determine the best penalty rate
- (Yuxin) Random Forest
  - pass along several possible combinations of hyper-parameters and perform cross validation
    - maximum depth, minimum number of samples at each split, n_estimators (4 for each)
    - parallel computing
- (Jen) K-Nearest Neighbors Regression
  - cross validation to determine the best k
- (Yuxin) XG-Boost
  - pass along several possible combinations of hyper-parameters and perform cross validation
  - maximum depth, minimum number of samples at each split, n_estimators (4 for each)
  - parallel computing
  
TARGET VARIABLE: Disaster_Frequency

graphics: 
- (Rene) line graph, count of disaster by year
  - color code by sector (t.s. plot)
- (Rene) line graph of aggregated count of disasters cross years. 
  - t.s. plot
- (Jen) world map and co2 emission (optional)
  - aggregating the co2 emission across all years, and rank by country.
  - ranking is going to be done by color. (gradient color, red is worse, while green is better)
- (Jen) co2 emission by country (do first, if time then do map)
- (Jen) bar graphs of *df_long* category column, x_axis is year. 
  - facet wrap by category
- (Yuxin) scatter plot, x-axis will be co2 emission, y-axis will be count of disasters. 
  - because count of disaster goes by year, then we will need to aggregate the co2 emission by country and by year. so we sum all of the sectors and types of co2 emissions. 

# Visualizations
```{r Scatterplot}
# optimize data for scatterplot
scatterplot_df <- df_long %>% group_by(ISO3, Year) %>%
  reframe(emission = sum(`Value`),
          Disaster_Frequency = first(Disaster_Frequency)) %>%
  glimpse()

# convert country (IOS3) as factor
scatterplot_df$ISO3 <- as.factor(scatterplot_df$ISO3)

# plot the scatter plot
scatterplot_df %>% ggplot(aes(x = log(emission + 0.0001), 
                              y = Disaster_Frequency)) + 
  geom_point() + 
  geom_point(data = scatterplot_df[scatterplot_df$Disaster_Frequency>10, ],
             aes(x = log(emission + 0.0001), 
                 y = Disaster_Frequency, 
                 color = ISO3)) +
  facet_wrap(~Year) +
  ylab("Disaster Count") + 
  xlab("Log of Emission (Log of Metric Tons of CO2)") +
  ggtitle("Emission versus Disaster Count for Each Country")
```

```{r Disaster By Year for Each Country}
df_aggregate_1 <- df_long %>% 
  group_by(Year, ISO3) %>% 
  summarise(total_disaster = sum(Disaster_Frequency)) %>%
  filter(total_disaster > 500)

# convert Year to numeric
df_aggregate_1$Year <- as.numeric(as.character(df_aggregate_1$Year))

# ggplot using aggregate dataset
ggplot(df_aggregate_1, aes(x = Year, y = total_disaster, color = ISO3)) +
  geom_line() +
  geom_point() +
  labs(title = "Count of Disaster by Year for Each Country (disasters > 500)",
       x = "Year", 
       y = "Sum of Disaster",
       color = "Country") 
```

```{r Disaster By Year}
df_aggregate_2 <- df_wide %>% 
  group_by(Year) %>% 
  summarize(total_disaster_frequency = sum(Disaster_Frequency))

# converting Year to numeric
df_aggregate_2$Year <- as.numeric(as.character(df_aggregate_2$Year))

# ggplot using aggregate dataset
ggplot(df_aggregate_2, aes(x = Year, y = total_disaster_frequency)) +
  geom_line(color = "blue") +
  geom_point(color = "blue") +
  labs(title = "Total Disaster Frequency by Year ",
       x = "Year",
       y = "Sum Disaster Frequency")
```

```{r CO2 Emissions by Type pf Enterprise}
df_agg <- df_long %>% 
  group_by(ISO3, Category) %>%
  reframe(emission = sum(`Value`)) %>%
  mutate_if(is.character, as.factor) %>%
  group_by(Category) %>%
  arrange(desc(emission)) %>%
  slice(1:10)

# CO2 by Country
ggplot(df_agg,aes(x = ISO3, y = emission, fill = Category)) + 
  geom_bar(stat = "identity")  +
  # facet_wrap(~Category)+
  labs(
    title = "CO2 emissions by Category of Enterprise for Each Country",
    x = "Country", 
    y = "Metric Tons of CO2 Emission"
  ) 
```

```{r Map of CO2 Emisions}
library(maps)
world <- map_data("world")

bind <- df_long %>% left_join(Country_label, by="ISO3") %>% 
  select(c(Country,Value)) %>% 
  group_by(Country) %>% reframe(Value = sum(Value)) %>% 
  # group_by(Country) %>% first(Value) %>% 
  glimpse()
world <- world %>% full_join(bind, by = c("region" ="Country"))
world <- world %>% mutate_all(~ ifelse(is.na(.), 0, .))

ggplot(world, aes(long, lat, group=group, fill = log(Value+1))) +
  geom_polygon(color="gray") +
  scale_fill_gradient(low = "white", high = "red") +
  coord_fixed() + 
  ggtitle("Map of CO2 Emission Recorded in Dataset") +
  labs(fill='Log of Metric \nTons of CO2 \nEmission')
```

# Modeling
## Random Forest
```{r Random Forest}
# set the seed
set.seed(123457)
# define parsnip, set random forest with mtry, trees, and min_n hyperparameters
# as tuning. To be tuned later.
parsnip_RF <- rand_forest(mtry = tune("mtry"),
                          trees = tune("trees"),
                          min_n = tune("min_n")) %>%
  set_engine('randomForest') %>%
  set_mode('regression') #set random forest to regression

# define random forest recipe, code all nominal predictors dummy variables and 
# normalize all numeric predictors.
RF_recipe <- 
  recipe(formula = Disaster_Frequency ~ ., data = train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors())

# define workflow
workflow_RF <- workflow() %>% 
  add_model(parsnip_RF) %>%
  add_recipe(RF_recipe)

# define hyperparameter tuning grid to be experimented
hyperparam_tune_grid <- crossing(min_n = seq(10, 50, by = 20),
                                 mtry = seq(10, 50, by = 20),
                                 trees = c(100, 500, 1000))
# define cross validation
rf_cv <- train %>% vfold_cv(v = 10, times = 2)
# define metrics to be used
RF_metrics <- metric_set(rmse, mae, rsq_trad)
```

```{r Random Forest Continued, eval = FALSE}
# assigning resources
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
time1 <- Sys.time() #save current system time

#run the 10 fold cross validation for each combination of hyperparameters
rf_tuning <- workflow_RF %>% 
  tune_grid(resamples = rf_cv, 
            grid = hyperparam_tune_grid,
            metrics = RF_metrics) %>%
  collect_metrics()

time2 <- Sys.time() #save current system time

(diff <- time2 - time1) #obtain total run time

stopCluster(cl)

save(rf_tuning, file = "RandomForest_Prediction.rds")
```

```{r Random Forest Result}
RF_prediction <- load("RandomForest_Prediction.rds")
rf_tuning 

# selecting the best hyperparameter combination
Best_result1 <- rf_tuning %>% 
  filter(!(.metric == "rsq_trad")) %>%
  group_by(.metric) %>% 
  slice_min(mean)
Best_result2 <- rf_tuning %>% 
  filter(.metric == "rsq_trad") %>%
  group_by(.metric) %>% 
  slice_max(mean)
Best_result <- Best_result1 %>% rbind(Best_result2)
Best_result
```

## XG-Boosting
```{r XG-Boosting}
# set the seed
set.seed(123457)
# define parsnip, set random forest with mtry, trees, and min_n hyperparameters
# as tuning. To be tuned later.
parsnip_boost <- boost_tree(tree_depth = tune("tree_depth"), 
                            learn_rate = tune("learn_rate"), 
                            min_n = tune("min_n")) %>%
  set_engine('xgboost') %>%
  set_mode('regression')

# define recipe for boosting, one hot encode dummy variable for nominal predictors
# and normalize all numeric predictors. 
boost_recipe <- 
  recipe(formula = Disaster_Frequency ~ ., data = train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_numeric_predictors())


# define workflow
workflow_boost <- workflow() %>% 
  add_model(parsnip_boost) %>%
  add_recipe(boost_recipe)

# define hyperparameter tuning grid to be experimented
hyperparam_tune_grid_boost <- crossing(min_n = seq(10, 50, by = 20),
                                       tree_depth = c(5, 10, 15),
                                       learn_rate = c(0.01, 0.05, 0.1))
# define cross validation
boost_cv <- train %>% vfold_cv(v = 10, times = 2)
# define metrics to be used
boost_metrics <- metric_set(rmse, mae, rsq_trad)
```

```{r XG-Boosting Continued, eval=FALSE}
# assigning resources
cl <- makePSOCKcluster(6)
registerDoParallel(cl)
time1 <- Sys.time() #save current system time

#run the 10 fold cross validation for each combination of hyperparameters
boost_tuning <- workflow_boost %>% 
  tune_grid(resamples = boost_cv, 
            grid = hyperparam_tune_grid_boost,
            metrics = boost_metrics) %>%
  collect_metrics()

time2 <- Sys.time() #save current system time

(diff <- time2 - time1) #obtain total run time

stopCluster(cl)

save(boost_tuning, file = "XGBoost_Prediction.rds")
```

```{r XG-Boost Result}
Boost_prediction <- load("XGBoost_Prediction.rds")
boost_tuning 

Best_result3 <- boost_tuning %>% 
  filter(!(.metric == "rsq_trad")) %>%
  group_by(.metric) %>% 
  slice_min(mean)
Best_result4 <- boost_tuning %>% 
  filter(.metric == "rsq_trad") %>%
  group_by(.metric) %>% 
  slice_max(mean)
Best_result5 <- Best_result3 %>% rbind(Best_result4)
Best_result5

# Fitting the train data with best hyperparameter combination
# all metrics support the combination of min_n = 10, 
# tree_depth = 10, and learn_rate = 0.1, we use it as our model for train set. 
parsnip_Boost_train <- boost_tree(tree_depth = 10, 
                                  learn_rate = 0.1, 
                                  min_n = 10) %>%
  set_engine('xgboost') %>%
  set_mode('regression')

# define workflow
workflow_Boost_train <- workflow() %>% 
  add_model(parsnip_Boost_train) %>%
  add_recipe(boost_recipe)

# fit to the train data
boost_fit <- workflow_Boost_train %>% fit(data = train) 
boost_fit 
```

## Poisson Generalized Linear Regression Model with Lasso Penalty
```{r Poisson Lasso}
# Poisson Generalized Linear Regression Model with Lasso Penalty
poi_recipe <- recipe(Disaster_Frequency ~., data = train_sub) %>%
  step_dummy(all_factor_predictors()) %>%
  step_normalize(all_predictors())
# defining model using poisson_reg()
poi_parsnip <-  poisson_reg(penalty = "lasso") %>%
  set_mode("regression") %>%
  set_engine("glm")
# defining workflow with model and recipe
poi_workflow <- workflow() %>%
  add_model(poi_parsnip) %>%
  add_recipe(poi_recipe)
```

```{r Poisson Lasso}
# Cross validation on train dataset
poi_result <- poi_workflow %>%
  fit_resamples(
    resamples = vfold_cv(train_sub, v = 10),
    metrics = metric_set(rmse, rsq, rsq_trad))
    
```

```{r Poisson Lasso Results}
# showing results 
poi_result %>% collect_metrics()

poi_fitted <- poi_workflow %>% fit(train_sub)
poi_fitted
```

## Poisson Generalized Linear Regression Model with Elastic Net Penalty
```{r Poisson Elastic Net}
#  Poisson Generalized Linear Model with Elastic Net Penalty
set.seed(123457)
# defining recipe
poi_recipe_enp <- recipe(Disaster_Frequency ~., data = train_sub) %>%
  step_dummy(all_factor_predictors()) %>%
  step_normalize(all_predictors())
# defining model using poisson_reg with elastic net penalty
poi_parsnip_enp <-  poisson_reg(penalty = "elastic_net") %>%
  set_mode("regression") %>%
  set_engine("glm")
# defining workflow with model and recipe
poi_workflow_enp <- workflow() %>%
  add_model(poi_parsnip_enp) %>%
  add_recipe(poi_recipe_enp)
```

```{r Poisson Elastic Net}
# cross validation on train dataset
poi_result_enp <- poi_workflow_enp %>%
  fit_resamples(
    resamples = vfold_cv(train_sub, v = 10),
    metrics = metric_set(rmse, rsq, rsq_trad),
    )
```

```{r Poisson Elastic Net Results}
# showing results
poi_result_enp %>% collect_metrics()
```

##  K-Nearest Neighbors Regression
```{r K-nearest}
# Set seed
set.seed(123457)
# k=5
knn_parsnip_5 <- nearest_neighbor() %>% 
  set_mode("regression") %>%
  set_engine("kknn", neighbors = 5)
knn_recipe_5 <- recipe(Disaster_Frequency ~ .,
                       data = train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())
knn_workflow_5 <- workflow() %>%
  add_model(knn_parsnip_5) %>%
  add_recipe(knn_recipe_5)
knn_5s <- knn_workflow_5 %>% fit(train)

# k=10
knn_parsnip_10 <- nearest_neighbor() %>% 
  set_mode("regression") %>%
  set_engine("kknn", neighbors = 10)
knn_recipe_10 <- recipe(Disaster_Frequency ~ .,
                        data = train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())
knn_workflow_10 <- workflow() %>%
  add_model(knn_parsnip_10) %>%
  add_recipe(knn_recipe_10)
knn_10 <- knn_workflow_10 %>% fit(train)

# k=20 
knn_parsnip_20 <- nearest_neighbor() %>% 
  set_mode("regression") %>%
  set_engine("kknn", neighbors = 20)
knn_recipe_20 <- recipe(Disaster_Frequency ~ .,
                        data = train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())
knn_workflow_20 <- workflow() %>%
  add_model(knn_parsnip_20) %>%
  add_recipe(knn_recipe_20)
knn_20 <- knn_workflow_20 %>% fit(train)
```

```{r K-nearest Validation}
# Cross validation
# k=5
knn_val1<- knn_workflow_5 %>%
  fit_resamples(
    resamples = vfold_cv(train, v = 10),
    metrics = metric_set(rmse, rsq, mae,rsq_trad),
  )
knn_val1 %>% collect_metrics()
# k=10
knn_val2 <- knn_workflow_10 %>%
  fit_resamples(
    resamples = vfold_cv(train, v = 10),
    metrics = metric_set(rmse, rsq, mae,rsq_trad),
  )
knn_val2 %>% collect_metrics()

# k=20
knn_val3 <- knn_workflow_10 %>%
  fit_resamples(
    resamples = vfold_cv(train, v = 10),
    metrics = metric_set(rmse, rsq, mae,rsq_trad),
  )
knn_val3 %>% collect_metrics()


metrics_knn <- bind_rows(
  knn_val1 %>% collect_metrics(),
  knn_val2 %>% collect_metrics(),
  knn_val3 %>% collect_metrics(),
  .id = "Model"
)
metrics_knn

```

